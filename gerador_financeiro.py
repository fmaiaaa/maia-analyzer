# -*- coding: utf-8 -*-
"""Gerador_financeiro.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e3FZIj7K-SnNqoY3QOBtiTbApRnnKKsQ
"""

import pandas as pd
import numpy as np
import yfinance as yf
from datetime import datetime, timedelta
from tqdm import tqdm
from arch import arch_model
import ta
from ta.trend import SMAIndicator, EMAIndicator, MACD, ADXIndicator, CCIIndicator
from ta.momentum import RSIIndicator, StochasticOscillator, ROCIndicator, WilliamsRIndicator
from ta.volatility import BollingerBands, AverageTrueRange, KeltnerChannel, DonchianChannel
from ta.volume import OnBalanceVolumeIndicator, ChaikinMoneyFlowIndicator, MFIIndicator, VolumeWeightedAveragePrice
from sklearn.preprocessing import RobustScaler
import time
import sys
import warnings
import os
from google.colab import auth
try:
    auth.authenticate_user()
    print('Autentica√ß√£o no Google Cloud bem-sucedida.')
except Exception as e:
    print(f'Falha na autentica√ß√£o: {e}')

# üö® IMPORTA√á√ÉO NECESS√ÅRIA PARA GCS
from google.cloud import storage

warnings.filterwarnings('ignore')

# =============================================================================
# üéØ VARI√ÅVEIS DE CONFIGURA√á√ÉO DO GCS (O QUE VOC√ä PRECISA MUDAR)
# =============================================================================
GCS_BUCKET_NAME = 'meu-portfolio-dados-gratuitos'
GCS_DESTINATION_FOLDER = 'dados_financeiros_etl' # Pasta l√≥gica dentro do bucket

# =============================================================================
# CONSTANTES E LISTAS DE ATIVOS
# =============================================================================
MIN_DIAS_HISTORICO = 252 # M√≠nimo de dias para ser considerado (aprox. 1 ano)
TAXA_LIVRE_RISCO = 0.1075 # Taxa de Juros Anual (para c√°lculo de Sharpe)
PERIODO_DADOS = 'max' # Coleta o m√°ximo de dados poss√≠vel
OUTPUT_DIR = 'dados_por_ativo' # Mantido, mas n√£o ser√° usado para salvar localmente

# Listas de ativos
ATIVOS_IBOVESPA = [
    'ALOS3.SA', 'ABEV3.SA', 'ASAI3.SA', 'AESB3.SA', 'AZZA3.SA', 'B3SA3.SA',
    'BBSE3.SA', 'BBDC3.SA', 'BBDC4.SA', 'BRAP4.SA', 'BBAS3.SA', 'BRKM5.SA',
    'BRAV3.SA', 'BPAC11.SA', 'CXSE3.SA', 'CEAB3.SA', 'CMIG4.SA', 'COGN3.SA',
    'CPLE6.SA', 'CSAN3.SA', 'CPFE3.SA', 'CMIN3.SA', 'CURY3.SA', 'CVCB3.SA',
    'CYRE3.SA', 'DIRR3.SA', 'ELET3.SA', 'ELET6.SA', 'EMBR3.SA', 'ENGI11.SA',
    'ENEV3.SA', 'EGIE3.SA', 'EQTL3.SA', 'FLRY3.SA', 'GGBR4.SA', 'GOAU4.SA',
    'HAPV3.SA', 'HYPE3.SA', 'IGTI11.SA', 'IRBR3.SA', 'ISAE4.SA', 'ITSA4.SA',
    'ITUB4.SA', 'KLBN11.SA', 'RENT3.SA', 'LREN3.SA', 'MGLU3.SA', 'POMO4.SA',
    'MBRF3.SA', 'BEEF3.SA', 'MOTV3.SA', 'MRVE3.SA', 'MULT3.SA', 'NATU3.SA',
    'PCAR3.SA', 'PETR3.SA', 'PETR4.SA', 'RECV3.SA', 'PRIO3.SA', 'PSSA3.SA',
    'RADL3.SA', 'RAIZ4.SA', 'RDOR3.SA', 'RAIL3.SA', 'SBSP3.SA', 'SANB11.SA',
    'CSNA3.SA', 'SLCE3.SA', 'SMFT3.SA', 'SUZB3.SA', 'TAEE11.SA', 'VIVT3.SA',
    'TIMS3.SA', 'TOTS3.SA', 'UGPA3.SA', 'USIM5.SA', 'VALE3.SA', 'VAMO3.SA',
    'VBBR3.SA', 'VIVA3.SA', 'WEGE3.SA', 'YDUQ3.SA'
]

ATIVOS_POR_SETOR = {
    'Bens Industriais': ['NATU3.SA', 'AMOB3.SA', 'ISAE4.SA', 'BHIA3.SA', 'ZAMP3.SA', 'AERI3.SA', 'ICBR3.SA', 'DOTZ3.SA', 'GOLL3.SA', 'VIIA3.SA', 'ARML3.SA', 'MLAS3.SA', 'CBAV3.SA', 'TTEN3.SA', 'BRBI11.SA', 'REAG3.SA', 'ATEA3.SA', 'MODL4.SA', 'VITT3.SA', 'KRSA3.SA', 'CXSE3.SA', 'RIOS3.SA', 'HCAR3.SA', 'GGPS3.SA', 'MATD3.SA', 'ALLD3.SA', 'BLAU3.SA', 'ATMP3.SA', 'ASAI3.SA', 'JSLG3.SA', 'CMIN3.SA', 'ELMD3.SA', 'ORVR3.SA', 'OPCT3.SA', 'WEST3.SA', 'CSED3.SA', 'BMOB3.SA', 'JALL3.SA', 'TOKY3.SA', 'ESPA3.SA', 'VAMO3.SA', 'INTB3.SA', 'NGRD3.SA', 'AVLL3.SA', 'RRRP3.SA', 'ENJU3.SA', 'CASH3.SA', 'TFCO4.SA', 'CONX3.SA', 'GMAT3.SA', 'SEQL3.SA', 'PASS3.SA', 'BOAS3.SA', 'MELK3.SA', 'HBSA3.SA', 'SIMH3.SA', 'CURY3.SA', 'PLPL3.SA', 'PETZ3.SA', 'PGMN3.SA', 'LAVV3.SA', 'LJQQ3.SA', 'DMVF3.SA', 'SOMA3.SA', 'RIVA3.SA', 'AMBP3.SA', 'ALPK3.SA'],
    'Consumo C√≠clico': ['AZZA3.SA', 'ALOS3.SA', 'VIIA3.SA', 'RDNI3.SA', 'SLED4.SA', 'RSID3.SA', 'MNDL3.SA', 'LEVE3.SA', 'CTKA4.SA', 'MYPK3.SA', 'GRND3.SA', 'LCAM3.SA', 'CEAB3.SA', 'VSTE3.SA', 'CGRA3.SA', 'ESTR4.SA', 'DIRR3.SA', 'CTNM3.SA', 'ANIM3.SA', 'EVEN3.SA', 'AMAR3.SA', 'MOVI3.SA', 'JHSF3.SA', 'HBOR3.SA', 'PDGR3.SA', 'ARZZ3.SA', 'EZTC3.SA', 'ALPA3.SA', 'RENT3.SA', 'MRVE3.SA', 'MGLU3.SA', 'LREN3.SA', 'COGN3.SA', 'WHRL4.SA', 'TCSA3.SA', 'SMLS3.SA', 'SEER3.SA', 'HOOT4.SA', 'GFSA3.SA', 'YDUQ3.SA', 'CYRE3.SA', 'CVCB3.SA', 'SBFG3.SA'],
    'Consumo n√£o C√≠clico': ['PRVA3.SA', 'SMTO3.SA', 'MDIA3.SA', 'CAML3.SA', 'AGRO3.SA', 'BEEF3.SA', 'VIVA3.SA', 'CRFB3.SA', 'PCAR3.SA', 'NTCO3.SA', 'NATU3.SA', 'MRFG3.SA', 'JBSS3.SA', 'BRFS3.SA'],
    'Financeiro': ['CSUD3.SA', 'INBR31.SA', 'BIDI3.SA', 'BIDI4.SA', 'IGTI11.SA', 'IGTI3.SA', 'XPBR31.SA', 'TRAD3.SA', 'BSLI4.SA', 'BTTL3.SA', 'BPAR3.SA', 'SCAR3.SA', 'LPSB3.SA', 'BMGB4.SA', 'IGBR3.SA', 'GSHP3.SA', 'PSSA3.SA', 'CARD3.SA', 'BBRK3.SA', 'BRPR3.SA', 'BRSR6.SA', 'SANB4.SA', 'SANB3.SA', 'MULT3.SA', 'ITUB3.SA', 'ITUB4.SA', 'ALSO3.SA', 'BMIN3.SA', 'MERC4.SA', 'LOGG3.SA', 'ITSA4.SA', 'IRBR3.SA', 'PDTC3.SA', 'SYNE3.SA', 'BBDC4.SA', 'BBDC3.SA', 'BRML3.SA', 'APER3.SA', 'BBSE3.SA', 'BPAN4.SA', 'BBAS3.SA'],
    'Materiais B√°sicos': ['LAND3.SA', 'DEXP4.SA', 'RANI3.SA', 'PMAM3.SA', 'FESA4.SA', 'EUCA3.SA', 'SUZB3.SA', 'KLBN4.SA', 'KLBN3.SA', 'VALE3.SA', 'VALE5.SA', 'UNIP6.SA', 'UNIP5.SA', 'GOAU4.SA', 'DXCO3.SA', 'CSNA3.SA', 'BRKM6.SA', 'BRKM5.SA', 'BRAP4.SA', 'BRAP3.SA'],
    'Petr√≥leo, G√°s e Biocombust√≠veis': ['SRNA3.SA', 'VBBR3.SA', 'RAIZ4.SA', 'RECV3.SA', 'PRIO3.SA', 'OSXB3.SA', 'DMMO3.SA', 'RPMG3.SA', 'UGPA3.SA', 'PETR4.SA', 'PETR3.SA', 'ENAT3.SA'],
    'Sa√∫de': ['ONCO3.SA', 'VVEO3.SA', 'PARD3.SA', 'BIOM3.SA', 'BALM3.SA', 'PNVL3.SA', 'AALR3.SA', 'ODPV3.SA', 'RADL3.SA', 'QUAL3.SA', 'OFSA3.SA', 'HYPE3.SA', 'FLRY3.SA'],
    'Tecnologia da Informa√ß√£o': ['CLSA3.SA', 'LVTC3.SA', 'G2DI33.SA', 'IFCM3.SA', 'GOGL35.SA', 'LWSA3.SA', 'TOTS3.SA', 'LINX3.SA', 'POSI3.SA'],
    'Telecomunica√ß√µes': ['BRIT3.SA', 'FIQE3.SA', 'DESK3.SA', 'TIMS3.SA', 'VIVT3.SA', 'TELB4.SA', 'TELB3.SA'],
    'Utilidade P√∫blica': ['BRAV3.SA', 'AESB3.SA', 'MEGA3.SA', 'CEPE6.SA', 'CEED3.SA', 'EEEL4.SA', 'CASN4.SA', 'CEGR3.SA', 'CEBR3.SA', 'RNEW4.SA', 'COCE6.SA', 'CLSC4.SA', 'ALUP4.SA', 'ALUP3.SA', 'SAPR4.SA', 'SAPR3.SA', 'CPRE3.SA', 'CPLE5.SA', 'CPLE6.SA', 'CPLE3.SA', 'CPFE3.SA', 'CGAS3.SA', 'NEOE3.SA', 'TRPL4.SA', 'TRPL3.SA', 'EGIE3.SA', 'TAEE4.SA', 'TAEE3.SA', 'SBSP3.SA', 'GEPA4.SA', 'CESP6.SA', 'CMIG4.SA', 'CMIG3.SA', 'AFLT3.SA']
}

# Consolida√ß√£o FINAL de todos os ativos para coleta (sem duplicatas)
TODOS_ATIVOS = set(ATIVOS_IBOVESPA)
for ativos in ATIVOS_POR_SETOR.values():
    TODOS_ATIVOS.update(ativos)
ATIVOS_PARA_COLETA = sorted(list(TODOS_ATIVOS))
print(f"Total de ativos √∫nicos para coleta: {len(ATIVOS_PARA_COLETA)}")

# =============================================================================
# FUN√á√ïES DE FEATURE ENGINEERING (INALTERADAS)
# =============================================================================

def calcular_indicadores_tecnicos(hist):
    """Calcula a lista COMPLETA de indicadores t√©cnicos e features temporais."""
    df = hist.copy()
    df = df.drop(columns=['Adj Close', 'Dividends', 'Stock Splits'], errors='ignore')
    df['returns'] = df['Close'].pct_change()
    df['log_returns'] = np.log(df['Close'] / df['Close'].shift(1))
    df['volatility_20'] = df['returns'].rolling(window=20).std() * np.sqrt(252)
    df['volatility_60'] = df['returns'].rolling(window=60).std() * np.sqrt(252)
    df['volatility_252'] = df['returns'].rolling(window=252).std() * np.sqrt(252)
    for periodo in [5, 10, 20, 50, 100, 200]:
        df[f'sma_{periodo}'] = SMAIndicator(close=df['Close'], window=periodo).sma_indicator()
        df[f'ema_{periodo}'] = EMAIndicator(close=df['Close'], window=periodo).ema_indicator()
        weights = np.arange(1, periodo + 1)
        df[f'wma_{periodo}'] = df['Close'].rolling(periodo).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)
    for periodo in [20, 50]:
        wma_half_series = df['Close'].rolling(periodo // 2).apply(lambda x: np.dot(x, np.arange(1, len(x) + 1)) / np.arange(1, len(x) + 1).sum(), raw=True)
        wma_full_series = df['Close'].rolling(periodo).apply(lambda x: np.dot(x, np.arange(1, len(x) + 1)) / np.arange(1, len(x) + 1).sum(), raw=True)
        df[f'hma_{periodo}'] = (2 * wma_half_series - wma_full_series).rolling(int(np.sqrt(periodo))).mean()
    df['price_sma20_ratio'] = df['Close'] / df['sma_20']; df['price_sma50_ratio'] = df['Close'] / df['sma_50']
    df['price_sma200_ratio'] = df['Close'] / df['sma_200']; df['sma20_sma50_cross'] = (df['sma_20'] > df['sma_50']).astype(int)
    df['sma50_sma200_cross'] = (df['sma_50'] > df['sma_200']).astype(int); df['death_cross'] = (df['Close'] < df['sma_200']).astype(int)
    df['higher_high'] = ((df['High'] > df['High'].shift(1)) & (df['High'].shift(1) > df['High'].shift(2))).astype(int)
    df['lower_low'] = ((df['Low'] < df['Low'].shift(1)) & (df['Low'].shift(1) < df['Low'].shift(2))).astype(int)
    for periodo in [7, 14, 21, 28]: df[f'rsi_{periodo}'] = RSIIndicator(close=df['Close'], window=periodo).rsi()
    stoch = StochasticOscillator(high=df['High'], low=df['Low'], close=df['Close'], window=14, smooth_window=3)
    df['stoch_k'] = stoch.stoch(); df['stoch_d'] = stoch.stoch_signal()
    df['williams_r'] = WilliamsRIndicator(high=df['High'], low=df['Low'], close=df['Close'], lbp=14).williams_r()
    macd = MACD(close=df['Close'], window_slow=26, window_fast=12, window_sign=9)
    df['macd'] = macd.macd(); df['macd_signal'] = macd.macd_signal(); df['macd_diff'] = macd.macd_diff()
    macd_alt = MACD(close=df['Close'], window_slow=35, window_fast=5, window_sign=5)
    df['macd_alt'] = macd_alt.macd()
    df['momentum_10'] = ROCIndicator(close=df['Close'], window=10).roc(); df['momentum_20'] = ROCIndicator(close=df['Close'], window=20).roc()
    df['momentum_60'] = ROCIndicator(close=df['Close'], window=60).roc()
    bb = BollingerBands(close=df['Close'], window=20, window_dev=2)
    df['bb_width'] = bb.bollinger_wband(); df['bb_position'] = (df['Close'] - bb.bollinger_lband()) / (bb.bollinger_hband() - bb.bollinger_lband())
    kc = KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close'], window=20, window_atr=10)
    df['kc_upper'] = kc.keltner_channel_hband(); df['kc_lower'] = kc.keltner_channel_lband(); df['kc_middle'] = kc.keltner_channel_mband()
    df['kc_width'] = (df['kc_upper'] - df['kc_lower']) / df['kc_middle']
    dc = DonchianChannel(high=df['High'], low=df['Low'], close=df['Close'], window=20)
    df['dc_upper'] = dc.donchian_channel_hband(); df['dc_lower'] = dc.donchian_channel_lband(); df['dc_middle'] = dc.donchian_channel_mband()
    atr = AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close'], window=14)
    df['atr'] = atr.average_true_range(); df['atr_percent'] = (df['atr'] / df['Close']) * 100
    adx = ADXIndicator(high=df['High'], low=df['Low'], close=df['Close'], window=14)
    df['adx'] = adx.adx(); df['adx_pos'] = adx.adx_pos(); df['adx_neg'] = adx.adx_neg()
    df['cci'] = CCIIndicator(high=df['High'], low=df['Low'], close=df['Close'], window=20).cci()
    df['obv'] = OnBalanceVolumeIndicator(close=df['Close'], volume=df['Volume']).on_balance_volume()
    df['cmf'] = ChaikinMoneyFlowIndicator(high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume'], window=20).chaikin_money_flow()
    df['mfi'] = MFIIndicator(high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume'], window=14).money_flow_index()
    df['vwap'] = VolumeWeightedAveragePrice(high=df['High'], low=df['Low'], close=df['Close'], volume=df['Volume']).volume_weighted_average_price()
    cumulative_returns = (1 + df['returns']).cumprod(); running_max = cumulative_returns.expanding().max()
    df['drawdown'] = (cumulative_returns - running_max) / running_max; df['max_drawdown_252'] = df['drawdown'].rolling(252).min()
    for lag in [1, 5, 10, 20, 60]:
        df[f'close_lag_{lag}'] = df['Close'].shift(lag); df[f'returns_lag_{lag}'] = df['returns'].shift(lag); df[f'volume_lag_{lag}'] = df['Volume'].shift(lag)
    for window in [5, 20, 60]:
        df[f'returns_mean_{window}'] = df['returns'].rolling(window).mean(); df[f'returns_std_{window}'] = df['returns'].rolling(window).std()
        df[f'returns_skew_{window}'] = df['returns'].rolling(window).skew(); df[f'returns_kurt_{window}'] = df['returns'].rolling(window).kurt()
        df[f'volume_mean_{window}'] = df['Volume'].rolling(window).mean(); df[f'volume_std_{window}'] = df['Volume'].rolling(window).std()
    for lag in [1, 5, 10]:
        df[f'autocorr_{lag}'] = df['returns'].rolling(60).apply(lambda x: x.autocorr(lag=lag), raw=False)
    df['day_of_week'] = df.index.dayofweek; df['month'] = df.index.month; df['quarter'] = df.index.quarter
    df['day_of_month'] = df.index.day; df['week_of_year'] = df.index.isocalendar().week.astype(int)
    return df.dropna(subset=['Close', 'Volume'])

def calcular_features_fundamentalistas(info):
    """Extrai lista COMPLETA de features fundamentalistas (24 m√©tricas)"""
    return {
        'pe_ratio': info.get('trailingPE', np.nan), 'forward_pe': info.get('forwardPE', np.nan),
        'pb_ratio': info.get('priceToBook', np.nan), 'ps_ratio': info.get('priceToSalesTrailing12Months', np.nan),
        'peg_ratio': info.get('pegRatio', np.nan), 'ev_ebitda': info.get('enterpriseToEbitda', np.nan),
        'div_yield': info.get('dividendYield', 0) * 100 if info.get('dividendYield') else np.nan,
        'payout_ratio': info.get('payoutRatio', np.nan) * 100 if info.get('payoutRatio') else np.nan,
        'roe': info.get('returnOnEquity', np.nan) * 100 if info.get('returnOnEquity') else np.nan,
        'roa': info.get('returnOnAssets', np.nan) * 100 if info.get('returnOnAssets') else np.nan,
        'roic': info.get('returnOnCapital', np.nan) * 100 if info.get('returnOnCapital') else np.nan,
        'profit_margin': info.get('profitMargins', np.nan) * 100 if info.get('profitMargins') else np.nan,
        'operating_margin': info.get('operatingMargins', np.nan) * 100 if info.get('operatingMargins') else np.nan,
        'gross_margin': info.get('grossMargins', np.nan) * 100 if info.get('grossMargins') else np.nan,
        'debt_to_equity': info.get('debtToEquity', np.nan), 'current_ratio': info.get('currentRatio', np.nan),
        'quick_ratio': info.get('quickRatio', np.nan), 'revenue_growth': info.get('revenueGrowth', np.nan) * 100 if info.get('revenueGrowth') else np.nan,
        'earnings_growth': info.get('earningsGrowth', np.nan) * 100 if info.get('earningsGrowth') else np.nan,
        'market_cap': info.get('marketCap', np.nan), 'enterprise_value': info.get('enterpriseValue', np.nan),
        'beta': info.get('beta', np.nan), 'sector': info.get('sector', 'Unknown'), 'industry': info.get('industry', 'Unknown')
    }

def calcular_metricas_performance(df_historico):
    """Calcula m√©tricas de performance (Sharpe, Retorno, Volatilidade, Drawdown)"""
    returns = df_historico['returns'].dropna()
    if returns.empty or len(returns) < 50:
        return {'sharpe_ratio': np.nan, 'annual_return': np.nan, 'annual_volatility': np.nan, 'max_drawdown': np.nan}
    annual_return = returns.mean() * 252
    annual_volatility = returns.std() * np.sqrt(252)
    sharpe_ratio = (annual_return - TAXA_LIVRE_RISCO) / annual_volatility if annual_volatility > 0 else 0
    cumulative = (1 + returns).cumprod()
    running_max = cumulative.expanding().max()
    drawdown = (cumulative - running_max) / running_max
    max_drawdown = drawdown.min()
    return {
        'sharpe_ratio': sharpe_ratio, 'annual_return': annual_return,
        'annual_volatility': annual_volatility, 'max_drawdown': max_drawdown
    }

def coletar_dados_macro_e_fund(simbolos):
    """Processamento completo com GARCH, Macro e Limpeza Seletiva"""

    dados_macro = {}; indices = {'IBOV': '^BVSP', 'USD_BRL': 'BRL=X'}
    for nome, simbolo in indices.items():
        try:
            hist = yf.Ticker(simbolo).history(period=PERIODO_DADOS)
            if not hist.empty: dados_macro[nome] = hist['Close'].pct_change()
        except: dados_macro[nome] = pd.Series(dtype=float)

    dados_historicos = {}; dados_fundamentalistas = []
    ESSENTIAL_COLS = ['Close', 'Volume', 'Open', 'High', 'Low']
    ticker = simbolos[0]

    try:
        time.sleep(1.0 + np.random.rand())
        ticker_obj = yf.Ticker(ticker)
        MAX_RETRIES = 3
        RETRY_DELAY = 2
        hist = pd.DataFrame()
        info = {}

        for attempt in range(MAX_RETRIES):
            try:
                hist = ticker_obj.history(period=PERIODO_DADOS)
                info = ticker_obj.info
                if not hist.empty:
                    break
            except Exception as e:
                if attempt == MAX_RETRIES - 1:
                    raise Exception(f"Falha total ao coletar dados para {ticker} ap√≥s {MAX_RETRIES} tentativas: {e}")
                time.sleep(RETRY_DELAY * (attempt + 1))

        if hist.empty or len(hist) < MIN_DIAS_HISTORICO or not all(col in hist.columns for col in ESSENTIAL_COLS):
            return {}, []

        df = calcular_indicadores_tecnicos(hist)

        if 'returns' in df.columns:
            for nome, serie_macro in dados_macro.items():
                combined_df = pd.DataFrame({'asset_returns': df['returns'], 'macro_returns': serie_macro.reindex(df.index)}).dropna()
                if len(combined_df) > 60:
                    corr_rolling = combined_df['asset_returns'].rolling(60).corr(combined_df['macro_returns'])
                    df[f'corr_{nome.lower()}'] = corr_rolling.reindex(df.index)
                else:
                    df[f'corr_{nome.lower()}'] = np.nan

        garch_vol = np.nan
        if 'returns' in df.columns and len(df['returns'].dropna()) >= 100:
            try:
                returns_limpo = df['returns'].dropna() * 100
                modelo = arch_model(returns_limpo, vol='Garch', p=1, q=1, rescale=False)
                resultado = modelo.fit(disp='off', show_warning=False, options={'maxiter': 1000})
                previsao = resultado.forecast(horizon=1)
                garch_vol = np.sqrt(previsao.variance.values[-1, 0]) / 100 * np.sqrt(252)
            except:
                returns_std = df['returns'].std(); garch_vol = returns_std * np.sqrt(252) if returns_std > 0 else np.nan

        df['garch_volatility'] = garch_vol
        df['ticker'] = ticker

        COLUNAS_LIMPEZA_FIM = [c for c in ['sma_200', 'corr_ibov', 'garch_volatility'] if c in df.columns]
        df_limpo = df.dropna(subset=COLUNAS_LIMPEZA_FIM + ['Close', 'Volume', 'returns'])

        if len(df_limpo) < MIN_DIAS_HISTORICO * 0.7:
            return {}, []

        dados_historicos[ticker] = df_limpo

        features_fund = calcular_features_fundamentalistas(info); features_fund['Ticker'] = ticker
        metricas_perf = calcular_metricas_performance(df_limpo)
        for k, v in metricas_perf.items(): features_fund[k] = v
        dados_fundamentalistas.append(features_fund)

    except Exception as e:
        print(f"\n[ERRO GERAL DE PROCESSAMENTO] Falha ao coletar/processar {ticker}: {e}")
        return {}, []

    return dados_historicos, dados_fundamentalistas

def criar_dataframe_consolidado(simbolos_list_of_one):
    """Pipeline final para criar o DataFrame consolidado."""

    historicos, fundamentos_list = coletar_dados_macro_e_fund(simbolos_list_of_one)
    ticker = simbolos_list_of_one[0]

    if not historicos:
        return pd.DataFrame()

    df_historico_completo = historicos[ticker]

    df_fundamentalistas = pd.DataFrame(fundamentos_list).set_index('Ticker')
    df_fundamentalistas = df_fundamentalistas.replace([np.inf, -np.inf], np.nan)
    numeric_cols_fund = df_fundamentalistas.select_dtypes(include=[np.number]).columns

    for col in numeric_cols_fund:
        df_fundamentalistas[col] = df_fundamentalistas[col].fillna(0)

    scaler = RobustScaler()
    try:
        df_fundamentalistas[numeric_cols_fund] = scaler.fit_transform(df_fundamentalistas[numeric_cols_fund])
    except:
        pass

    cols_performance_sector = ['sharpe_ratio', 'annual_return', 'annual_volatility', 'max_drawdown', 'sector', 'industry']
    df_performance_sector = df_fundamentalistas[cols_performance_sector].copy()
    df_estaticas = df_fundamentalistas.drop(columns=cols_performance_sector, errors='ignore').add_prefix('fund_')

    df_final = df_historico_completo.reset_index().rename(columns={'index': 'Date'})

    df_final = df_final.merge(
        df_estaticas.reset_index(),
        left_on='ticker',
        right_on='Ticker',
        how='left'
    ).drop(columns=['Ticker'])

    df_final = df_final.merge(
        df_performance_sector.reset_index(),
        left_on='ticker',
        right_on='Ticker',
        how='left'
    )
    df_final = df_final.drop(columns=['Ticker']).set_index(['Date', 'ticker']).sort_index()

    return df_final

# =============================================================================
# üöÄ NOVO: FUN√á√ÉO DE EXPORTA√á√ÉO PARA GCS
# =============================================================================

def upload_dataframe_to_gcs(df: pd.DataFrame, bucket_name: str, blob_name: str):
    """
    Salva um DataFrame no GCS como um arquivo CSV usando buffer em mem√≥ria.
    """
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    # Define o caminho completo dentro do bucket
    blob = bucket.blob(blob_name)

    # Converte o DataFrame para CSV em mem√≥ria
    csv_data = df.to_csv(index=True)

    # Faz o upload dos dados
    blob.upload_from_string(csv_data, content_type='text/csv')

    return f"gs://{bucket_name}/{blob_name}"

# =============================================================================
# EXECU√á√ÉO DO PIPELINE (ADAPTADA PARA GCS)
# =============================================================================

print(f"INICIANDO ETL FULL por ativo. Os arquivos ser√£o salvos no GCS em **gs://{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}**.")

# A cria√ß√£o de pasta local (os.makedirs(OUTPUT_DIR, exist_ok=True)) foi removida.
sucesso_contagem = 0

for ticker in tqdm(ATIVOS_PARA_COLETA, desc="Processando Ativos (Total)"):

    # Define o caminho do arquivo no GCS
    blob_path = f'{GCS_DESTINATION_FOLDER}/{ticker}.csv'

    try:
        # 4. Cria o DataFrame consolidado APENAS para este ativo
        df_consolidado_ativo = criar_dataframe_consolidado([ticker])

        if df_consolidado_ativo.empty:
            continue

        # 5. üöÄ EXPORTA√á√ÉO PARA GCS
        gcs_uri = upload_dataframe_to_gcs(
            df_consolidado_ativo,
            GCS_BUCKET_NAME,
            blob_path
        )
        sucesso_contagem += 1

    except Exception as e:
        print(f"‚ùå ERRO CR√çTICO na gera√ß√£o/upload do ativo {ticker}: {e}")

print("\n" + "="*70)
print(f"‚úÖ FIM DO PROCESSO. {sucesso_contagem} arquivos CSV foram gerados no GCS em gs://{GCS_BUCKET_NAME}/{GCS_DESTINATION_FOLDER}.")
print("="*70)